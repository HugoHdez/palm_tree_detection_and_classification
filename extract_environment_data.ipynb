{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "67c32663",
   "metadata": {},
   "source": [
    "En este notebook, se cargan los datos y comprobaremos el numero de ambientes de palmeras que existen para saber como estructurar el dataset. Además, se entrenarán tanto al modelo de YOLO como ResNet50."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "904fd4f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import os\n",
    "import json\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68313586",
   "metadata": {},
   "source": [
    "Se cuentan cuantas palmeras existen por categoría de entorno en GC, TNF y La Gomera"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7703ad37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ruta del archivo Shapefile con las coordenadas de las palmeras\n",
    "shapefile_path = r\"F:\\Universidad\\Curso 2024-25\\Segundo Semestre\\TFG\\Desarrollo\\Mapa_IDECanarias_Palmeras/99_K_Mapapalmerascanarias.shp\"\n",
    "\n",
    "# Cargar y leer el archivo\n",
    "data = gpd.read_file(shapefile_path)\n",
    "\n",
    "# Filtrar solo las palmeras de Gran Canaria\n",
    "data_gc_tnf_gm = data[data[\"ISLA\"].isin([\"GRAN CANARIA\", \"TENERIFE\", \"LA GOMERA\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "17e7ad6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recuento de palmeras por tipo de ambiente en Gran Canaria, Tenerife y La Gomera:\n",
      "Tip_amb\n",
      "?        382\n",
      "ot      4985\n",
      "vi     25521\n",
      "rd     25846\n",
      "ud     48391\n",
      "ru     81281\n",
      "uj     85765\n",
      "na    123900\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Contar el número de palmeras por tipo de ambiente\n",
    "environment_counts = data_gc_tnf_gm.value_counts(\"Tip_amb\", ascending=True)\n",
    "\n",
    "# Mostrar los resultados\n",
    "print(\"Recuento de palmeras por tipo de ambiente en Gran Canaria, Tenerife y La Gomera:\")\n",
    "print(environment_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7177bb1a",
   "metadata": {},
   "source": [
    "Entrenamiento y validación del modelo clasificador de ambientes con YOLO-cls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "641edc95",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "# Cargamos el modelo YOLOv11 de clasificación preentrenado\n",
    "model = YOLO(\"yolo11m-cls.pt\")\n",
    "\n",
    "# Entrenamos el modelo con las imágenes clasificadas por ambiente\n",
    "model.train(\n",
    "    data=r\"F:\\Universidad\\Curso 2024-25\\Segundo Semestre\\TFG\\Desarrollo\\dataset\\classification_per_type\\environment_classification_dataset\",\n",
    "    epochs=100,\n",
    "    patience=10,\n",
    "    imgsz=256,\n",
    "    batch=16,\n",
    "    device=0,\n",
    "    name=\"yolo_env_train\",\n",
    "    project=r\".\\classification\\env\\yolov11m\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "018158cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = r\".\\classification\\env\\yolov11m\\yolo_env_train\\weights\\best.pt\"\n",
    "validation_path = r\"F:\\Universidad\\Curso 2024-25\\Segundo Semestre\\TFG\\Desarrollo\\dataset\\classification_per_type\\environment_classification_dataset\"\n",
    "model = YOLO(model_path)\n",
    "metrics = model.val(data=validation_path, split=\"val\", project=r\".\\classification\\env\\yolov11m\", name=\"yolo_env_val\", plots=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3aad802",
   "metadata": {},
   "source": [
    "Entrenamos con resnet50 para hacer una comparación con el modelo de Yolo11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b2de317",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import models, datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "dataset_dir = r\"F:\\Universidad\\Curso 2024-25\\Segundo Semestre\\TFG\\Desarrollo\\dataset\\classification_per_type\\environment_classification_dataset\"\n",
    "dir_to_save_metrics = r\"classification\\env\\resnet50\\metrics\"\n",
    "dir_to_save_model = r\"classification\\env\\resnet50\\resnet50_best_final.pth\"\n",
    "num_classes = len(os.listdir(os.path.join(dataset_dir, \"train\")))\n",
    "batch_size = 32\n",
    "num_epochs = 100\n",
    "# Tasa de aprendizaje\n",
    "lr = 0.001\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Parametros para Early Stopping\n",
    "patience = 10\n",
    "# Conteo de épocas sin mejora\n",
    "epochs_without_improvement = 0\n",
    "# Mejor precisión de validación para guardar el modelo con mejor generalización\n",
    "best_val_acc = 0.0\n",
    "\n",
    "# Transforms para el entrenamiento con preprocesamiento y data augmentation\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(256, scale=(0.8, 1.0)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(15),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# Transforms para la validación\n",
    "transform_val = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# Cargar datos\n",
    "train_dataset = datasets.ImageFolder(os.path.join(dataset_dir, 'train'), transform=transform_train)\n",
    "val_dataset = datasets.ImageFolder(os.path.join(dataset_dir, 'val'), transform=transform_val)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Cargar modelo preentrenado\n",
    "model = models.resnet50(pretrained=True)\n",
    "\n",
    "# Congelar capas convolucionales\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Descongelar capa 'layer4'\n",
    "for name, param in model.named_parameters():\n",
    "    if \"layer4\" in name:\n",
    "        param.requires_grad = True\n",
    "\n",
    "# Regularización para evitar el sobreajuste\n",
    "model.fc = nn.Sequential(\n",
    "    nn.Dropout(p=0.6),  # 60% de las neuronas se apagan aleatoriamente durante el entrenamiento\n",
    "    nn.Linear(model.fc.in_features, num_classes)\n",
    ")\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "\n",
    "# Optimizador y scheduler\n",
    "optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=lr, weight_decay=1e-4)\n",
    "# Reduce la tasa de aprendizajesi no mejora la perdida de validación\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=1)\n",
    "# Suavizado de etiquetas para evitar el sobreajuste\n",
    "criterion = nn.CrossEntropyLoss(label_smoothing=0.1)  \n",
    "\n",
    "# Métricas por época\n",
    "train_losses, val_losses = [], []\n",
    "val_accuracies, val_f1s, val_precisions, val_recalls = [], [], [], []\n",
    "\n",
    "# Entrenamiento del modelo por épocas\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    avg_train_loss = train_loss / len(train_loader)\n",
    "    train_losses.append(avg_train_loss)\n",
    "\n",
    "    # Validación\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    all_preds, all_labels = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    val_losses.append(avg_val_loss)\n",
    "    scheduler.step(avg_val_loss)\n",
    "\n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    f1 = f1_score(all_labels, all_preds, average='macro')\n",
    "    precision = precision_score(all_labels, all_preds, average='macro')\n",
    "    recall = recall_score(all_labels, all_preds, average='macro')\n",
    "\n",
    "    val_accuracies.append(acc)\n",
    "    val_f1s.append(f1)\n",
    "    val_precisions.append(precision)\n",
    "    val_recalls.append(recall)\n",
    "\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} | Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f} | \"\n",
    "          f\"Val Acc: {acc:.4f} | F1: {f1:.4f} | Precision: {precision:.4f} | Recall: {recall:.4f}\")\n",
    "\n",
    "    # Early stopping\n",
    "    if acc > best_val_acc:\n",
    "        best_val_acc = acc\n",
    "        torch.save(model.state_dict(), dir_to_save_model)\n",
    "        epochs_without_improvement = 0\n",
    "    else:\n",
    "        epochs_without_improvement += 1\n",
    "        if epochs_without_improvement >= patience:\n",
    "            print(\"Early stopping\")\n",
    "            break\n",
    "\n",
    "# Cargar el mejor modelo guardado\n",
    "print(\"Cargando el modelo:\")\n",
    "model.load_state_dict(torch.load(dir_to_save_model))\n",
    "model.eval()\n",
    "\n",
    "# Evaluación final en validación\n",
    "all_preds, all_labels = [], []\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in val_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "# Métricas finales\n",
    "final_acc = accuracy_score(all_labels, all_preds)\n",
    "final_f1 = f1_score(all_labels, all_preds, average='macro')\n",
    "final_precision = precision_score(all_labels, all_preds, average='macro')\n",
    "final_recall = recall_score(all_labels, all_preds, average='macro')\n",
    "\n",
    "print(\"Evaluación final del mejor modelo guardado:\")\n",
    "print(f\"   Accuracy:  {final_acc:.4f}\")\n",
    "print(f\"   F1 Macro:  {final_f1:.4f}\")\n",
    "print(f\"   Precision: {final_precision:.4f}\")\n",
    "print(f\"   Recall:    {final_recall:.4f}\")\n",
    "\n",
    "\n",
    "if not os.path.exists(dir_to_save_metrics):\n",
    "    os.makedirs(dir_to_save_metrics)\n",
    "\n",
    "# Guardar métricas\n",
    "df_log = pd.DataFrame({\n",
    "    'epoch': list(range(1, len(train_losses)+1)),\n",
    "    'train_loss': train_losses,\n",
    "    'val_loss': val_losses,\n",
    "    'val_accuracy': val_accuracies,\n",
    "    'val_f1_macro': val_f1s,\n",
    "    'val_precision': val_precisions,\n",
    "    'val_recall': val_recalls\n",
    "})\n",
    "df_log.to_csv(os.path.join(dir_to_save_metrics, \"training_metrics_log.csv\"), index=False)\n",
    "\n",
    "# Graficar métricas\n",
    "plt.figure()\n",
    "plt.plot(df_log['epoch'], df_log['train_loss'], label='Train Loss')\n",
    "plt.plot(df_log['epoch'], df_log['val_loss'], label='Val Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training vs Validation Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.savefig(os.path.join(dir_to_save_metrics, \"loss_curve.png\"))\n",
    "plt.close()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(df_log['epoch'], df_log['val_accuracy'], label='Validation Accuracy')\n",
    "plt.plot(df_log['epoch'], df_log['val_f1_macro'], label='F1 Macro')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Score')\n",
    "plt.title('Validation Accuracy & F1 Macro Over Epochs')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.savefig(os.path.join(dir_to_save_metrics, \"accuracy_f1_curve.png\"))\n",
    "plt.close()\n",
    "\n",
    "class_names = val_dataset.classes\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False, xticklabels=class_names, yticklabels=class_names)\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.savefig(os.path.join(dir_to_save_metrics, \"confusion_matrix.png\"))\n",
    "plt.close()\n",
    "\n",
    "# Matriz de confusión normalizada (por fila)\n",
    "cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm_normalized, annot=True, fmt=\".2f\", cmap='Blues', cbar=True, xticklabels=class_names, yticklabels=class_names)\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.title(\"Confusion Matrix (Normalized by True Labels)\")\n",
    "plt.savefig(os.path.join(dir_to_save_metrics, \"confusion_matrix_normalized.png\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TFG_yolov11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
