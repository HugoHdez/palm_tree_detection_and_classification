{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "093238eb",
   "metadata": {},
   "source": [
    "I this Jupyter notebook, we will load the data and retrieve the number of palms types existing to know how to divide the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "683e894d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import os\n",
    "import json\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63e53115",
   "metadata": {},
   "source": [
    "We will count how many palms exists per type category in GC, TNF and La Gomera"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ac2adf9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ruta del archivo Shapefile con las coordenadas de las palmeras\n",
    "shapefile_path = r\"F:\\Universidad\\Curso 2024-25\\Segundo Semestre\\TFG\\Desarrollo\\Mapa_IDECanarias_Palmeras/99_K_Mapapalmerascanarias.shp\"\n",
    "\n",
    "# Cargar y leer el archivo\n",
    "data = gpd.read_file(shapefile_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "26467d45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Id_palm Tip_amb                       Tip_amb_de Hibrid  \\\n",
      "132957  Cx454646y3100878       ?  Ambientes de difícil asignación     pc   \n",
      "132958  Cx454653y3100874       ?  Ambientes de difícil asignación     pc   \n",
      "132959  Cx454659y3100870       ?  Ambientes de difícil asignación     pc   \n",
      "132960  Cx454677y3100862       ?  Ambientes de difícil asignación     pc   \n",
      "132961  Cx454672y3100865       ?  Ambientes de difícil asignación     pc   \n",
      "132962  Cx454666y3100868       ?  Ambientes de difícil asignación     pc   \n",
      "132963  Cx454607y3100765       ?  Ambientes de difícil asignación     pc   \n",
      "132964  Cx454547y3100787       ?  Ambientes de difícil asignación     pc   \n",
      "132965  Cx454758y3100800       ?  Ambientes de difícil asignación     pc   \n",
      "132966  Cx454745y3100790       ?  Ambientes de difícil asignación     pc   \n",
      "\n",
      "                Hibrid_de          ISLA      MUNICIPIO  \\\n",
      "132957  Palmeras canarias  GRAN CANARIA  SANTA BRÍGIDA   \n",
      "132958  Palmeras canarias  GRAN CANARIA  SANTA BRÍGIDA   \n",
      "132959  Palmeras canarias  GRAN CANARIA  SANTA BRÍGIDA   \n",
      "132960  Palmeras canarias  GRAN CANARIA  SANTA BRÍGIDA   \n",
      "132961  Palmeras canarias  GRAN CANARIA  SANTA BRÍGIDA   \n",
      "132962  Palmeras canarias  GRAN CANARIA  SANTA BRÍGIDA   \n",
      "132963  Palmeras canarias  GRAN CANARIA  SANTA BRÍGIDA   \n",
      "132964  Palmeras canarias  GRAN CANARIA  SANTA BRÍGIDA   \n",
      "132965  Palmeras canarias  GRAN CANARIA  SANTA BRÍGIDA   \n",
      "132966  Palmeras canarias  GRAN CANARIA  SANTA BRÍGIDA   \n",
      "\n",
      "                              geometry  \n",
      "132957  POINT (454646.195 3100878.293)  \n",
      "132958  POINT (454653.074 3100873.663)  \n",
      "132959  POINT (454659.424 3100870.488)  \n",
      "132960  POINT (454677.416 3100862.022)  \n",
      "132961  POINT (454672.124 3100865.329)  \n",
      "132962   POINT (454665.51 3100868.239)  \n",
      "132963  POINT (454607.433 3100765.052)  \n",
      "132964  POINT (454546.711 3100786.747)  \n",
      "132965  POINT (454757.585 3100799.712)  \n",
      "132966  POINT (454745.017 3100790.055)  \n",
      "Recuento de palmeras por tipo en Gran Canaria, Tenerife y La Gomera:\n",
      "Hibrid\n",
      "?           3\n",
      "tdi       591\n",
      "pme       878\n",
      "da        888\n",
      "hme      1885\n",
      "bdc      4162\n",
      "pc     387664\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Filtrar solo las palmeras de Gran Canaria\n",
    "data_gc_tnf_gm = data[data[\"ISLA\"].isin([\"GRAN CANARIA\", \"TENERIFE\", \"LA GOMERA\"])].copy()\n",
    "\n",
    "# Convertimos los valores None de la columna \"Hibrid\" a PC (Palmera Canaria), que quiere decir que no se presenta hibridación\n",
    "data_gc_tnf_gm[\"Hibrid\"] = data_gc_tnf_gm[\"Hibrid\"].fillna(\"pc\")\n",
    "\n",
    "# Contar el número de palmeras por tipo de ambiente\n",
    "palm_type_counts = data_gc_tnf_gm.value_counts(\"Hibrid\", ascending=True)\n",
    "\n",
    "# Eliminamos del dataframe las palmeras con hibridaje de dificil asignación o \"?\"\n",
    "data_gc_tnf_gm = data_gc_tnf_gm[data_gc_tnf_gm[\"Hibrid\"] != \"?\"]\n",
    "\n",
    "print(data_gc_tnf_gm.head(10))\n",
    "\n",
    "# Mostrar los resultados\n",
    "print(\"Recuento de palmeras por tipo en Gran Canaria, Tenerife y La Gomera:\")\n",
    "print(palm_type_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aea18673",
   "metadata": {},
   "source": [
    "Entrenamiento y validación del modelo de YOLO-cls para clasificación de tipos de palmeras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72b939c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "# Cargamos el modelo YOLOv11 de clasificación preentrenado\n",
    "model = YOLO(\"yolo11m-cls.pt\")\n",
    "\n",
    "# Entrenamos el modelo con las imágenes clasificadas por ambiente\n",
    "model.train(\n",
    "    data=r\"F:\\Universidad\\Curso 2024-25\\Segundo Semestre\\TFG\\Desarrollo\\dataset\\classification_per_type\\palm_type_classification_dataset\",\n",
    "    epochs=100,\n",
    "    patience=10,\n",
    "    imgsz=32,\n",
    "    batch=64,\n",
    "    device=0,\n",
    "    plots = True,\n",
    "    name=\"yolo_type_train\",\n",
    "    project=r\".\\classification\\type\\yolov11m\",\n",
    "    # Mejora de generalización con estos parámetros\n",
    "    label_smoothing=0.1,     # Suavizado de etiquetas\n",
    "    weight_decay=1e-4,       # Penalización L2\n",
    "    freeze=[0, 1, 2],        # Congela capas backbone más profundas\n",
    "    mixup=0.2,               # Aplica mixup con probabilidad 20%\n",
    "    cutmix=0.2,              # Aplica cutmix con probabilidad 20%\n",
    "    augment=True             # Aplica aumentos por defecto\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24d6a6b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = r\".\\classification\\env\\yolov11m\\yolo_type_train\\weights\\best.pt\"\n",
    "validation_path = r\"F:\\Universidad\\Curso 2024-25\\Segundo Semestre\\TFG\\Desarrollo\\dataset\\classification_per_type\\palm_type_classification_dataset\"\n",
    "model = YOLO(model_path)\n",
    "metrics = model.val(data=validation_path, split=\"val\", project=r\".\\classification\\type\\yolov11m\", name=\"yolo_type_val\", plots=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "527b6e99",
   "metadata": {},
   "source": [
    "Entrenamiento y validación del modelo de ResNet50 para clasificación de tipos de palmeras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c452b718",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import models, datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "dataset_dir = r\"F:\\Universidad\\Curso 2024-25\\Segundo Semestre\\TFG\\Desarrollo\\dataset\\classification_per_type\\palm_type_classification_dataset\"\n",
    "dir_to_save_metrics = r\"classification\\type\\resnet50\\metrics\"\n",
    "dir_to_save_model = r\"classification\\type\\resnet50\\resnet50_best_final.pth\"\n",
    "num_classes = len(os.listdir(os.path.join(dataset_dir, 'train')))\n",
    "batch_size = 32\n",
    "num_epochs = 50\n",
    "lr = 0.001\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Parametros para Early Stopping\n",
    "patience = 10\n",
    "epochs_without_improvement = 0\n",
    "best_val_acc = 0.0\n",
    "\n",
    "# Transforms para entrenamiento con data augmentation\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(256, scale=(0.8, 1.0)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(15),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# Transformador para validación\n",
    "transform_val = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# Cargar datos\n",
    "train_dataset = datasets.ImageFolder(os.path.join(dataset_dir, 'train'), transform=transform_train)\n",
    "val_dataset = datasets.ImageFolder(os.path.join(dataset_dir, 'val'), transform=transform_val)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Cargar modelo preentrenado\n",
    "model = models.resnet50(pretrained=True)\n",
    "\n",
    "# Congelar capas convolucionales\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Descongelar capa 'layer4'\n",
    "for name, param in model.named_parameters():\n",
    "    if \"layer4\" in name:\n",
    "        param.requires_grad = True\n",
    "\n",
    "# Regularización para evitar el sobreajuste\n",
    "model.fc = nn.Sequential(\n",
    "    nn.Dropout(p=0.6),  # 60% de las neuronas se apagan aleatoriamente durante el entrenamiento\n",
    "    nn.Linear(model.fc.in_features, num_classes)\n",
    ")\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "\n",
    "# Optimizador y scheduler que ajusta tasa de aprendizaje a medida que avanza el entrenamiento\n",
    "optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=lr, weight_decay=1e-4)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=1)\n",
    "# Suavizado de etiquetas para evitar el sobreajuste\n",
    "criterion = nn.CrossEntropyLoss(label_smoothing=0.1)  \n",
    "\n",
    "# Métricas por época\n",
    "train_losses, val_losses = [], []\n",
    "val_accuracies, val_f1s, val_precisions, val_recalls = [], [], [], []\n",
    "\n",
    "best_val_acc = 0.0\n",
    "epochs_without_improvement = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "\n",
    "    # Entrenamiento por lotes\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    avg_train_loss = train_loss / len(train_loader)\n",
    "    train_losses.append(avg_train_loss)\n",
    "\n",
    "    # Validación\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    all_preds, all_labels = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    val_losses.append(avg_val_loss)\n",
    "    scheduler.step(avg_val_loss)\n",
    "\n",
    "    # Cálculo de métricas\n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    f1 = f1_score(all_labels, all_preds, average='macro')\n",
    "    precision = precision_score(all_labels, all_preds, average='macro')\n",
    "    recall = recall_score(all_labels, all_preds, average='macro')\n",
    "\n",
    "    val_accuracies.append(acc)\n",
    "    val_f1s.append(f1)\n",
    "    val_precisions.append(precision)\n",
    "    val_recalls.append(recall)\n",
    "\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} | Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f} | \"\n",
    "          f\"Val Acc: {acc:.4f} | F1: {f1:.4f} | Precision: {precision:.4f} | Recall: {recall:.4f}\")\n",
    "\n",
    "    # Early stopping\n",
    "    if acc > best_val_acc:\n",
    "        best_val_acc = acc\n",
    "        torch.save(model.state_dict(), dir_to_save_model)\n",
    "        epochs_without_improvement = 0\n",
    "    else:\n",
    "        epochs_without_improvement += 1\n",
    "        if epochs_without_improvement >= patience:\n",
    "            print(\"Early stopping\")\n",
    "            break\n",
    "\n",
    "# Cargar el mejor modelo guardado\n",
    "print(\"Cargando el modelo:\")\n",
    "model.load_state_dict(torch.load(dir_to_save_model))\n",
    "model.eval()\n",
    "\n",
    "# Evaluación final en validación\n",
    "all_preds, all_labels = [], []\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in val_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "# Métricas finales\n",
    "final_acc = accuracy_score(all_labels, all_preds)\n",
    "final_f1 = f1_score(all_labels, all_preds, average='macro')\n",
    "final_precision = precision_score(all_labels, all_preds, average='macro')\n",
    "final_recall = recall_score(all_labels, all_preds, average='macro')\n",
    "\n",
    "print(\"Evaluación final del mejor modelo guardado:\")\n",
    "print(f\"   Accuracy:  {final_acc:.4f}\")\n",
    "print(f\"   F1 Macro:  {final_f1:.4f}\")\n",
    "print(f\"   Precision: {final_precision:.4f}\")\n",
    "print(f\"   Recall:    {final_recall:.4f}\")\n",
    "\n",
    "\n",
    "if not os.path.exists(dir_to_save_metrics):\n",
    "    os.makedirs(dir_to_save_metrics)\n",
    "\n",
    "# Guardar métricas\n",
    "df_log = pd.DataFrame({\n",
    "    'epoch': list(range(1, len(train_losses)+1)),\n",
    "    'train_loss': train_losses,\n",
    "    'val_loss': val_losses,\n",
    "    'val_accuracy': val_accuracies,\n",
    "    'val_f1_macro': val_f1s,\n",
    "    'val_precision': val_precisions,\n",
    "    'val_recall': val_recalls\n",
    "})\n",
    "df_log.to_csv(os.path.join(dir_to_save_metrics, \"training_metrics_log.csv\"), index=False)\n",
    "\n",
    "# Graficar métricas\n",
    "plt.figure()\n",
    "plt.plot(df_log['epoch'], df_log['train_loss'], label='Train Loss')\n",
    "plt.plot(df_log['epoch'], df_log['val_loss'], label='Val Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training vs Validation Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.savefig(os.path.join(dir_to_save_metrics, \"loss_curve.png\"))\n",
    "plt.close()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(df_log['epoch'], df_log['val_accuracy'], label='Validation Accuracy')\n",
    "plt.plot(df_log['epoch'], df_log['val_f1_macro'], label='F1 Macro')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Score')\n",
    "plt.title('Validation Accuracy & F1 Macro Over Epochs')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.savefig(os.path.join(dir_to_save_metrics, \"accuracy_f1_curve.png\"))\n",
    "plt.close()\n",
    "\n",
    "class_names = val_dataset.classes\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False, xticklabels=class_names, yticklabels=class_names)\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.savefig(os.path.join(dir_to_save_metrics, \"confusion_matrix.png\"))\n",
    "plt.close()\n",
    "\n",
    "# Matriz de confusión normalizada (por fila)\n",
    "cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm_normalized, annot=True, fmt=\".2f\", cmap='Blues', cbar=True, xticklabels=class_names, yticklabels=class_names)\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.title(\"Confusion Matrix (Normalized by True Labels)\")\n",
    "plt.savefig(os.path.join(dir_to_save_metrics, \"confusion_matrix_normalized.png\"))\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "261717d1",
   "metadata": {},
   "source": [
    "Entrenamiento y validación con MobileNet para la clasificación de tipos de palmeras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d83fd8f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms, models\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Configuración\n",
    "dataset_dir = r\"F:\\\\Universidad\\\\Curso 2024-25\\\\Segundo Semestre\\\\TFG\\\\Desarrollo\\\\dataset\\\\classification_per_type\\\\palm_type_classification_dataset\"\n",
    "dir_to_save_metrics = r\"classification\\type\\mobilenet\\metrics\"\n",
    "dir_to_save_model = r\"classification\\type\\mobilenet\\mobilenet.pth\"\n",
    "os.makedirs(dir_to_save_metrics, exist_ok=True)\n",
    "num_classes = len(os.listdir(os.path.join(dataset_dir, 'train')))\n",
    "batch_size = 32\n",
    "num_epochs = 100\n",
    "lr = 0.001\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "patience = 10\n",
    "\n",
    "# Tranformadores para el entrenamiento (con data augmentation) y validación\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.Resize((96, 96)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(20),\n",
    "    transforms.RandomAffine(degrees=0, translate=(0.1, 0.1), scale=(0.9, 1.1)),\n",
    "    transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.2),\n",
    "    transforms.GaussianBlur(kernel_size=3),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "transform_val = transforms.Compose([\n",
    "    transforms.Resize((96, 96)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Datasets y Loaders\n",
    "train_dataset = datasets.ImageFolder(os.path.join(dataset_dir, 'train'), transform=transform_train)\n",
    "val_dataset = datasets.ImageFolder(os.path.join(dataset_dir, 'val'), transform=transform_val)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "class_names = val_dataset.classes\n",
    "\n",
    "# Pesos de clase\n",
    "labels = [sample[1] for sample in train_dataset.samples]\n",
    "class_weights = compute_class_weight(class_weight=\"balanced\", classes=np.unique(labels), y=labels)\n",
    "class_weights_tensor = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
    "\n",
    "\n",
    "# Modelo MobileNet\n",
    "model = models.mobilenet_v2(pretrained=True)\n",
    "\n",
    "# Congelar solo capas iniciales\n",
    "for name, param in model.features.named_parameters():\n",
    "    if int(name.split('.')[0]) < 5:\n",
    "        param.requires_grad = False\n",
    "\n",
    "model.classifier = nn.Sequential(\n",
    "    nn.Dropout(0.2),    # Dropout del 20% para evitar el sobreajuste\n",
    "    nn.Linear(model.last_channel, num_classes)\n",
    ")\n",
    "model = model.to(device)\n",
    "\n",
    "# Optimizador y scheduler que ajusta tasa de aprendizaje a medida que avanza el entrenamiento\n",
    "optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=5e-5)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=1)\n",
    "# Función de pérdida\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights_tensor)\n",
    "\n",
    "# Métricas\n",
    "train_losses, val_losses = [], []\n",
    "val_accuracies, val_f1s, val_precisions, val_recalls = [], [], [], []\n",
    "best_val_acc = 0.0\n",
    "epochs_without_improvement = 0\n",
    "\n",
    "# Entrenamiento por épocas\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    avg_train_loss = train_loss / len(train_loader)\n",
    "    train_losses.append(avg_train_loss)\n",
    "\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    all_preds, all_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    val_losses.append(avg_val_loss)\n",
    "    scheduler.step(avg_val_loss)\n",
    "\n",
    "    # Cálculo de métricas\n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    f1 = f1_score(all_labels, all_preds, average='macro')\n",
    "    precision = precision_score(all_labels, all_preds, average='macro', zero_division=0)\n",
    "    recall = recall_score(all_labels, all_preds, average='macro', zero_division=0)\n",
    "\n",
    "    val_accuracies.append(acc)\n",
    "    val_f1s.append(f1)\n",
    "    val_precisions.append(precision)\n",
    "    val_recalls.append(recall)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} | Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f} | \"\n",
    "          f\"Val Acc: {acc:.4f} | F1: {f1:.4f} | Precision: {precision:.4f} | Recall: {recall:.4f}\")\n",
    "\n",
    "    if acc > best_val_acc:\n",
    "        best_val_acc = acc\n",
    "        torch.save(model.state_dict(), dir_to_save_model)\n",
    "        epochs_without_improvement = 0\n",
    "    else:\n",
    "        epochs_without_improvement += 1\n",
    "        if epochs_without_improvement >= patience:\n",
    "            print(\"Early stopping\")\n",
    "            break\n",
    "\n",
    "# Evaluación final\n",
    "print(\"Cargando el mejor modelo guardado...\")\n",
    "model.load_state_dict(torch.load(dir_to_save_model))\n",
    "model.eval()\n",
    "all_preds, all_labels = [], []\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in val_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "final_acc = accuracy_score(all_labels, all_preds)\n",
    "final_f1 = f1_score(all_labels, all_preds, average='macro')\n",
    "final_precision = precision_score(all_labels, all_preds, average='macro')\n",
    "final_recall = recall_score(all_labels, all_preds, average='macro')\n",
    "print(\"Evaluación final:\")\n",
    "print(f\"   Accuracy:  {final_acc:.4f}\")\n",
    "print(f\"   F1 macro:  {final_f1:.4f}\")\n",
    "print(f\"   Precision: {final_precision:.4f}\")\n",
    "print(f\"   Recall:    {final_recall:.4f}\")\n",
    "\n",
    "# Guardado de métricas\n",
    "if not os.path.exists(dir_to_save_metrics):\n",
    "    os.makedirs(dir_to_save_metrics)\n",
    "\n",
    "df_log = pd.DataFrame({\n",
    "    'epoch': list(range(1, len(train_losses)+1)),\n",
    "    'train_loss': train_losses,\n",
    "    'val_loss': val_losses,\n",
    "    'val_accuracy': val_accuracies,\n",
    "    'val_f1_macro': val_f1s,\n",
    "    'val_precision': val_precisions,\n",
    "    'val_recall': val_recalls\n",
    "})\n",
    "df_log.to_csv(os.path.join(dir_to_save_metrics, \"training_metrics_log.csv\"), index=False)\n",
    "\n",
    "\n",
    "# Gráficas\n",
    "plt.figure()\n",
    "plt.plot(df_log['epoch'], df_log['train_loss'], label='Train Loss')\n",
    "plt.plot(df_log['epoch'], df_log['val_loss'], label='Val Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training vs Validation Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.savefig(os.path.join(dir_to_save_metrics, \"loss_curve.png\"))\n",
    "plt.close()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(df_log['epoch'], df_log['val_accuracy'], label='Validation Accuracy')\n",
    "plt.plot(df_log['epoch'], df_log['val_f1_macro'], label='F1 macro')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Score')\n",
    "plt.title('Validation Accuracy & F1 macro Over Epochs')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.savefig(os.path.join(dir_to_save_metrics, \"accuracy_f1_curve.png\"))\n",
    "plt.close()\n",
    "\n",
    "# Matriz de confusión\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=class_names, yticklabels=class_names)\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.savefig(os.path.join(dir_to_save_metrics, \"confusion_matrix.png\"))\n",
    "plt.close()\n",
    "\n",
    "# Matriz normalizada\n",
    "cm_norm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm_norm, annot=True, fmt='.2f', cmap='Blues',\n",
    "            xticklabels=class_names, yticklabels=class_names)\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.title(\"Confusion Matrix (Normalized)\")\n",
    "plt.savefig(os.path.join(dir_to_save_metrics, \"confusion_matrix_normalized.png\"))\n",
    "plt.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TFG_yolov11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
